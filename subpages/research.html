<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf=8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="description" content="Personal Website">
        <meta name="keywords" content="Aidan Good, Computer Science, Recall Distortion">
        <meta name="author" content="Aidan Good">
        <link rel="shortcut icon" type="image/x-icon" href="../favicon.ico">
        <title>Aidan Good | 2022 Paper</title>
        <link rel="stylesheet" href="../css/style.css">
    </head>
    <body>
        <div class="main-container">
            <header>
                <div class="container">
                    <div id="FrontName">
                        <h1><span class="highlight">William</span> Aidan Good</h1>
                    </div>
                    <nav>
                        <u1>
                            <li><a href="../index.html">Home</a></li>
                            <li><a href="../about.html">About</a></li>
                            <li><a href="../projects.html">Projects</a></li>
                        </u1>
                    </nav>
                </div>
            </header>

            <div id="mainSubpage">
                <div class="container">
                    <h1 id="title">Recall Distortion in Neural Network Pruning <br> and the Undecayed Pruning Algorithm</h1>
                    <p>
                        Paper acepted into the NeurIPS 2022 Conference. <a href="https://arxiv.org/abs/2206.02976">Link to the main paper on arXiv.</a> <br>
                        Also accepted into the 2022 Sparsity in Neural Networks workshop. <br>
                        <a href="../SNN_2022_Poster.pdf">Click here to view PDF of Poster</a> <br>
                    </p>

                    <p>
                        <b><u>Abstract:</u></b> Pruning techniques have been successfully used in neural networks to trade accuracy for sparsity.
                         However, this impact is not uniform: prior work has shown the recall for underrepresented classes in a dataset may be more negatively affected. 
                         In this work, we study such relative distortions in recall by hypothesizing an intensification effect. 
                         In addition, we propose a new pruning algorithm aimed at attenuating such effect. 
                         Through statistical analysis, we have observed that intensification is less severe with our algorithm but nevertheless more pronounced with relatively more difficult tasks, less complex models, and higher pruning ratios.
                        
                    </p>
                    
                    <p>
                        I starting working on this project in the summer of 2021 with Professor Thiago Serra and 3 other students. Initially set out to statistically measure if there were fairness issues introduced 
                        by classical pruning methods, we eventually shifted focus to finding evidence of the existence of an "intensification effect", where class recalls above model accuracy before pruning get relatively better,
                        and class recalls below model accuracy before pruning get relatively worse, after pruning. 
                    </p>

                    <p>
                        To remove all the academic and mathematical fluff from the paper: working with "modern" vision neural network architectures (show the network a 
                        picture of a dog and it can identify it is a dog), I found that pruning these networks (removing bits and pieces, done for several benefits like faster inferencing, smaller footprint, etc.) can introduce 
                        a bias against certain classes. To quantify this, we created a metric called the <i>Intensification Ratio</i>. This measures when class recalls above model accuracy before pruning get relatively better,
                        and class recalls below model accuracy before pruning get relatively worse, after pruning. We then statistically show that with randomly initialized models, this effect exists for certain 
                        architectures, datasets, and pruning strategies. Suprisingly, at moderate pruning rates (50- 75%) we observe an opposite effect than what we expected, where the classes initially worse before pruning 
                        actually improve after pruning. 
                    </p>

                    <p>
                        There already exists a growing field of research in the fairness issues around neural network pruning, possibly the most well-known being the work of
                        Sara Hooker. She and her team had already created a metric to quantify this issue, PIEs (or CIEs depending on which paper you read). The PIE metric counts the number of images classified differently by the
                        pruned neural network compared to the original network. And here in my opinion is why our metric, the Intensificaion Ratio, is better. Using PIEs, it says that anything different is counted against that
                        pruned model, regardless of whether the original model <i>got it wrong</i> and the pruned model later <i>got it right</i>. See the issue? Our metric, however, is able to highlight when this situation occurs,
                        and is how can make the observation that at moderate prunning rates (50 - 75%) we see fairness actaully improving with the models we studied.
                    </p>
                    
                </div>
            </div>
        </div>
    </body>
</html>